{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, subprocess, re\n",
    "from functools import reduce\n",
    "\n",
    "containing_folder = \"./SAMPLE_FILES/\"\n",
    "filename = \"USES.txt\"\n",
    "wordnet_strategy = \"hypen\" #\"synsn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sense:\n",
    "    def __init__(self, sense_string, wn_strategy=\"synsn\"):\n",
    "        self.wn_strategy = wn_strategy\n",
    "        if wn_strategy==\"synsn\":\n",
    "            '''\n",
    "            self.synonyms = {'jump', 'skip', 'run', ...}\n",
    "            '''\n",
    "            ret = re.split(\"\\=\\>\", sense_string)\n",
    "            self.synonyms = ret.pop(0)\n",
    "            self.categories = ret\n",
    "            self.categories = [c.strip() for c in self.categories]\n",
    "            self.synonyms = set(self.synonyms.strip().split(\",\"))\n",
    "\n",
    "        if wn_strategy.startswith(\"hype\"):\n",
    "            '''\n",
    "            self.layers = [\n",
    "                subsense [..., ['whole', 'unit'], ['object', 'physical object'], ['entity']],\n",
    "                subsense [..., [...], [...]]\n",
    "            ]\n",
    "            '''\n",
    "            self.layers = []\n",
    "            lines = sense_string.split(\"\\n\")\n",
    "\n",
    "            # remove empty lines\n",
    "            for i in range(len(lines)-1, -1, -1):\n",
    "                if lines[i].strip() == \"\":\n",
    "                    lines.pop(i)\n",
    "\n",
    "\n",
    "            current_layer = []\n",
    "            for i, line in enumerate(lines):\n",
    "                if (i>0):\n",
    "                    if (Sense.GetNestingSize(lines[i-1]) > Sense.GetNestingSize(line)):\n",
    "                        self.layers.append(current_layer)\n",
    "                        current_layer = []\n",
    "\n",
    "                line = line.strip(\" =>\")\n",
    "                values = re.split(\",[ ]*\", line)\n",
    "                current_layer.append(values)\n",
    "            self.layers.append(current_layer)\n",
    "            self.category = self.layers.pop(0)[0][0]\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Unknown Wordnet Strategy:\", wn_strategy)\n",
    "\n",
    "    @staticmethod\n",
    "    def GetNestingSize(line):\n",
    "        return len(re.split(\" \\=\\>\", line)[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def list_distance(a, b, return_match=False):\n",
    "        distances = []\n",
    "        for a_sense in a:\n",
    "            for b_sense in b:\n",
    "                distances.append(a_sense.distance(b_sense, return_match))\n",
    "\n",
    "        if return_match:\n",
    "            distances.sort(key=lambda x: x[0])\n",
    "            return distances[0]\n",
    "        else:\n",
    "            distances.sort()\n",
    "            return distances[0]\n",
    "\n",
    "    def distance(self, other, return_match=False):\n",
    "        if self.wn_strategy.startswith(\"hype\"):\n",
    "            matches = []\n",
    "            for outter_layer in self.layers:\n",
    "                for i, outter_value_set in enumerate(outter_layer):\n",
    "                    for layer in other.layers:\n",
    "                        for j, value_set in enumerate(layer):\n",
    "                            if value_set == outter_value_set:\n",
    "                                matches.append((i+j, value_set))\n",
    "\n",
    "            matches.sort(key=lambda x: x[0])\n",
    "            if return_match:\n",
    "                return matches[0]\n",
    "            else:\n",
    "                return matches[0][0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.wn_strategy == \"synsn\":\n",
    "            return \"Sense with type {} and synonyms {}\".format(self.categories, self.synonyms)\n",
    "        if self.wn_strategy.startswith(\"hype\"):\n",
    "            return \"Sense of {} with layer [{}, {}, {}, ..., {}, {}]\".format(\n",
    "                self.category, self.layers[0][0][0], self.layers[0][1][0], self.layers[0][2][0], self.layers[0][-2][0], self.layers[0][-1][0]\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __contains__(self, other):\n",
    "        if self.wn_strategy == \"synsn\":\n",
    "            return other in self.synonyms\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(list(self.synonyms))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_word(word):\n",
    "        proc = subprocess.run([\"wn {} -{}\".format(word, wordnet_strategy)], capture_output=True, shell=True)\n",
    "        output = proc.stdout.decode()\n",
    "        \n",
    "        sense_strings = re.split(\"Sense [0-9]\", output)\n",
    "        sense_strings.pop(0)\n",
    "        print(\"Found\", len(sense_strings), \"Senses for\", word)\n",
    "\n",
    "        senses = []\n",
    "        for string in sense_strings:\n",
    "            senses.append(Sense(string, wordnet_strategy))\n",
    "\n",
    "        return senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 2 Senses for elephant\nFound 2 Senses for computer\n[Sense of elephant with layer [proboscidean, placental, mammal, ..., physical entity, entity], Sense of elephant with layer [emblem, symbol, representational process, ..., abstraction, entity]]\n[Sense of computer with layer [machine, device, instrumentality, ..., physical entity, entity], Sense of calculator with layer [expert, person, organism, ..., physical entity, entity]]\n\n==> Smallest Distance between elephant, computer: 8\n\tmatched on: ['organism', 'being']\n"
     ]
    }
   ],
   "source": [
    "word_a = \"elephant\"\n",
    "word_b = \"computer\"\n",
    "\n",
    "# Get senses for each word\n",
    "a = Sense.check_word(word_a)\n",
    "b = Sense.check_word(word_b)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# Get Distance\n",
    "distance = Sense.list_distance(a, b) # returns an int\n",
    "detailed_distance = Sense.list_distance(a, b, return_match=True) # return_match returns a tuple: (distance, matching phrase)\n",
    "\n",
    "print(\"\\n==> Smallest Distance between {}, {}: {}\".format(word_a, word_b, distance))\n",
    "print(\"\\tmatched on:\", detailed_distance[1])\n"
   ]
  },
  {
   "source": [
    "### Hey-o Wait up\n",
    "Notice that the second sense of *computer* has something to do with the old school definition of a computer as a *person* who does calculations? I'm not sure any of our participants had this sense of a computer in thier mind at any point in time. If this is the case, we could get rid of this sense of a computer and just use the machine type of computer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sense of calculator with layer [expert, person, organism, ..., physical entity, entity]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Remove human computer sense\n",
    "b.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> Smallest Distance between 'elephant' and 'computer': 12\n\tmatched on: ['whole', 'unit']\n"
     ]
    }
   ],
   "source": [
    "# Recalculate\n",
    "detailed_distance = Sense.list_distance(a, b, True)\n",
    "print(\"==> Smallest Distance between '{}' and '{}': {}\\n\\tmatched on: {}\".format(word_a, word_b, detailed_distance[0], detailed_distance[1]))"
   ]
  },
  {
   "source": [
    "# Old Stuff\n",
    "Using a synonym strategy for finding distance. This was getting annoyingly complicated so I switched over to hypernyms."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking computer processor\nFound 3 Senses for processor\n[Sense of processor with layer [business, enterprise, organization, ..., abstraction, entity], Sense of processor with layer [worker, person, organism, ..., physical entity, entity], Sense of central processing unit with layer [electronic equipment, equipment, instrumentality, ..., physical entity, entity]]\nFound 2 Senses for computer\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Sense' object has no attribute 'synonyms'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f8ef574f4e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mfind_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f8ef574f4e31>\u001b[0m in \u001b[0;36mfind_distance\u001b[0;34m(base, target)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbase_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtarget_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f8ef574f4e31>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(senses)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mto_synonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msenses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f8ef574f4e31>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mto_synonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msenses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sense' object has no attribute 'synonyms'"
     ]
    }
   ],
   "source": [
    "base = \"computer\"\n",
    "target = \"processor\"\n",
    "\n",
    "def find_distance(base, target):\n",
    "    to_synonyms = lambda senses: reduce(lambda x, y: {*x.synonyms, *y.synonyms}, senses)\n",
    "    print(\"Checking\", base, target)\n",
    "    print(Sense.check_word(target))\n",
    "\n",
    "    base_syn = to_synonyms(Sense.check_word(base))\n",
    "    target_syn = to_synonyms(Sense.check_word(target))\n",
    "    inc = 0\n",
    "\n",
    "    while True:\n",
    "        print(\"Checking\", base, target, \"in\\n\\t\", base_syn, \"\\n\\t\", target_syn)\n",
    "        temp_target_syn = set()\n",
    "        temp_base_syn = set()\n",
    "\n",
    "        inc += 1\n",
    "        # Step forward one level in base word\n",
    "        for word in base_syn:\n",
    "            temp = to_synonyms(Sense.check_word(word))\n",
    "            temp_base_syn.union(temp)\n",
    "            if len(temp & target_syn) > 0 or target in temp:\n",
    "                return inc\n",
    "\n",
    "        # Step forward one level in target word\n",
    "        for word in target_syn:\n",
    "            temp = to_synonyms(Sense.check_word(word))\n",
    "            temp_target_syn.union(temp)\n",
    "            if len(temp & base_syn) > 0 or base in temp:\n",
    "                return inc\n",
    "\n",
    "        inc += 1\n",
    "        # Check if new steps caused overlap\n",
    "        if len(temp_target_syn & temp_base_syn) > 0:\n",
    "            return inc\n",
    "        else:\n",
    "            base_syn.union(temp_base_syn)\n",
    "            target_syn.union(temp_target_syn)\n",
    "\n",
    "\n",
    "find_distance(base, target)\n"
   ]
  }
 ]
}